{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Side Notes for Lesson 3\n",
    "\n",
    "### Tokenization\n",
    "\n",
    "What does it mean to 'tokenize' a phrase? Should we simply split on spaces? What are some of the subtleties?\n",
    "\n",
    "Some of the issues could be:\n",
    "* **Sparsity and vocabulary size:**\n",
    "   - are 'swim', 'swimming', 'run', 'running' four completely independent tokens?\n",
    "   - if those are fully independent then the vocabulary will need to be very large\n",
    "   \n",
    "* **Missing important relationships**\n",
    "   - the example above makes the point clear\n",
    "   \n",
    "* **Dealing with numbers**\n",
    "   - what about 'ranked place 7' vs '123 horses on the farm', vs '12765 BC was a rainy year'. Are we really reserving a token for each number that could show up?\n",
    " \n",
    "In the following we will look at three tokenizers and see how handle these issues: the tokenizers of Google BERT and Roberta, obtained from Huggingface (https://huggingface.co/), and NLTK's wordtokenizer. \n",
    "\n",
    "(Note: Both BERT and Roberta are pre-trained Transformer models that generate word embeddings (and more) in a sophisticated and context-aware way. (See Week 9, but it is ok to give a preview).)\n",
    "\n",
    "We start by importing the transformer library and some other useful ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, RobertaTokenizer, DistilBertTokenizer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "roberta_tokenizer =  RobertaTokenizer.from_pretrained('roberta-base') \n",
    "\n",
    "def show_tokenizations(phrase):\n",
    "    print('BERT:\\n\\t', bert_tokenizer.tokenize(phrase))\n",
    "    print('Roberta:\\n\\t', roberta_tokenizer.tokenize(phrase))\n",
    "    print('NLTK:\\n\\t', word_tokenize(phrase))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT:\n",
      "\t ['This', 'ship', 'was', 'built', '123', '##45', 'BC']\n",
      "Roberta:\n",
      "\t ['This', 'Ġship', 'Ġwas', 'Ġbuilt', 'Ġ123', '45', 'ĠBC']\n",
      "NLTK:\n",
      "\t ['This', 'ship', 'was', 'built', '12345', 'BC']\n"
     ]
    }
   ],
   "source": [
    "show_tokenizations(\"This ship was built 12345 BC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "\n",
    "* numbers are handled differently\n",
    "* 'continuation markers' are handled differently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT:\n",
      "\t ['The', 'dog', 'sniffed', 'the', 'food', '.', 'He', 'bunker', '##ed', 'it', '.']\n",
      "Roberta:\n",
      "\t ['The', 'Ġdog', 'Ġsniff', 'ed', 'Ġthe', 'Ġfood', '.', 'ĠHe', 'Ġbun', 'kered', 'Ġit', '.']\n",
      "NLTK:\n",
      "\t ['The', 'dog', 'sniffed', 'the', 'food', '.', 'He', 'bunkered', 'it', '.']\n"
     ]
    }
   ],
   "source": [
    "show_tokenizations(\"The dog sniffed the food. He bunkered it.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "\n",
    "* handling of past tense of less common verbs ('sniffed', 'bunkered')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT:\n",
      "\t ['Ser', '##end', '##ip', '##ity', 'is', 'on', 'lists', 'of', 'un', '##tra', '##ns', '##late', '##bal', '##e', 'words']\n",
      "Roberta:\n",
      "\t ['S', 'ere', 'nd', 'ip', 'ity', 'Ġis', 'Ġon', 'Ġlists', 'Ġof', 'Ġunt', 'rans', 'late', 'b', 'ale', 'Ġwords']\n",
      "NLTK:\n",
      "\t ['Serendipity', 'is', 'on', 'lists', 'of', 'untranslatebale', 'words']\n"
     ]
    }
   ],
   "source": [
    "show_tokenizations(\"Serendipity is on lists of untranslatebale words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "\n",
    "* Different word pieces in different tokenizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT:\n",
      "\t ['The', 'sky', 'is', 'blue', '.', 'So', 'is', 'the', 'ocean', '.', 'Both', 'are', 'blue', '.']\n",
      "Roberta:\n",
      "\t ['The', 'Ġsky', 'Ġis', 'Ġblue', '.', 'So', 'Ġis', 'Ġthe', 'Ġocean', '.', 'ĠBoth', 'Ġare', 'Ġblue', '.']\n",
      "NLTK:\n",
      "\t ['The', 'sky', 'is', 'blue.So', 'is', 'the', 'ocean', '.', 'Both', 'are', 'blue', '.']\n"
     ]
    }
   ],
   "source": [
    "show_tokenizations(\"The sky is blue.So is the ocean. Both are blue.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "\n",
    "* Subtleties with missing spaces etc. ('... blue.So is...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT:\n",
      "\t ['Swimming', ',', 'bi', '##king', 'are', 'fun', '.', 'Best', 'is', 'swimming', '.']\n",
      "Roberta:\n",
      "\t ['Sw', 'imming', ',', 'Ġbiking', 'Ġare', 'Ġfun', '.', 'ĠBest', 'Ġis', 'Ġswimming', '.']\n",
      "NLTK:\n",
      "\t ['Swimming', ',', 'biking', 'are', 'fun', '.', 'Best', 'is', 'swimming', '.']\n"
     ]
    }
   ],
   "source": [
    "show_tokenizations(\"Swimming, biking are fun. Best is swimming.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "\n",
    "* different splittings - see 'swimming' vs 'biking' for BERT and Roberta\n",
    "* Different handling of capitalized first token in sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT:\n",
      "\t ['Don', \"'\", 't', 'answer', 'me', '!']\n",
      "Roberta:\n",
      "\t ['Don', \"'t\", 'Ġanswer', 'Ġme', '!']\n",
      "NLTK:\n",
      "\t ['Do', \"n't\", 'answer', 'me', '!']\n"
     ]
    }
   ],
   "source": [
    "show_tokenizations(\"Don't answer me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "\n",
    "* different handling of \"don\\'t\\'-s\"\n",
    "\n",
    "Does all of that matter? YES!!\n",
    "\n",
    "* Model performance is affected\n",
    "* Code for how to stitch back together a sentence fro  tokens...\n",
    "* ...."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mids_ss_21",
   "language": "python",
   "name": "mids_ss_21"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
