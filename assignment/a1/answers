# Write your short answers in this file, replacing the placeholders as appropriate.
# This assignment consists of 3 parts for a total of 21 points.
# For numerical answers, copy and paste at least 5 significant figures.
# - Information Theory (18 points)
# - Async (1 points)
# - Readings Intro (2 points)



###################################################################
###################################################################
## Information Theory (18 points)
###################################################################
###################################################################


# ------------------------------------------------------------------
# | Section (Code): Binary Entropy (1 points)  | 
# ------------------------------------------------------------------

# Question 1 (/1): What is BinaryEntropy(0.86)?
information_theory_code_1: 0.00000


# ------------------------------------------------------------------
# | Section (A): Pointwise Mutual Information (2 points)  | 
# ------------------------------------------------------------------

# Question 1 (/1): What is PMI(rainy, cloudy)?
information_theory_a_1: 0.00000

# Question 2 (/1): What is PMI(washington, post)?
information_theory_a_2: 0.00000


# ------------------------------------------------------------------
# | Section (B): Entropy (5 points)  | 
# ------------------------------------------------------------------

# Question 1.1 (/1): Expected bits for uniform probability, 128 messages?
information_theory_b_1_1: 0

# Question 1.2 (/1): Entropy for uniform probability, 128 messages?
information_theory_b_1_2: 0

# Question 1.3 (/1): Entropy for uniform probability, 1024 messages?
information_theory_b_1_3: 0

# Question 2 (/1): Which has higher entropy?
# (This question is multiple choice.  Delete all but the correct answer).
information_theory_b_2: 
 - A
 - B

# Question 3 (/1): Which has higher entropy?
# (This question is multiple choice.  Delete all but the correct answer).
information_theory_b_3: 
 - A
 - B


# ------------------------------------------------------------------
# | Section (C): Cross-Entropy and KL Divergence (10 points)  | 
# ------------------------------------------------------------------

# Question 1 (/1): What is the Cross Entropy?
information_theory_c_1: 0.00000

# Question 2 (/1): What is the KL divergence?
information_theory_c_2: 0.00000

# Question 3 (/5): Do you actually need to compute everything?  (Hint... think of the value of most terms in the summation)
# (This question is multiple choice.  Delete all but the correct answer).
information_theory_c_3: 
 - Compute everything
 - Do not compute everything

# Question 4 (/1): What if the model put all the mass on the correct class?  What is the cross entropy?
# (This question is multiple choice.  Delete all but the correct answer).
information_theory_c_4: 
 - Zero
 - Infinite
 - Finite nonzero

# Question 5 (/1): What if the model put all the mass on class 1?  What is the cross entropy?
# (This question is multiple choice.  Delete all but the correct answer).
information_theory_c_5: 
 - Zero
 - Infinite
 - Finite nonzero

# Question 6 (/1): What if the model put 1/3 of the mass on classes 1, 2, 3?  What is the cross entropy?
# (This question is multiple choice.  Delete all but the correct answer).
information_theory_c_6: 
 - Zero
 - Infinite
 - Finite nonzero



###################################################################
###################################################################
## Async (1 points)
###################################################################
###################################################################


# ------------------------------------------------------------------
# | Section (a): Language Typology (1 points)  | 
# ------------------------------------------------------------------

# Question 1 (/1): According to async, which of the following is a word order that is NOT seen?
# (This question is multiple choice.  Delete all but the correct answer).
async_a_1: 
 - SVO
 - OVS
 - VSO



###################################################################
###################################################################
## Readings Intro (2 points)
###################################################################
###################################################################


# ------------------------------------------------------------------
# | Section (a): Readings (2 points)  | 
# ------------------------------------------------------------------

# Question 1 (/2): The problem with the dialog system is that?
# (This question is multiple choice.  Delete all but the correct answer).
readings_intro_a_1: 
 - It can generate coherent speech but it has no sense of humor.
 - It understands that certain patterns of symbols go together, but it doesn't have any common sense.
 - It can converse and stands on the verge of sentience.
